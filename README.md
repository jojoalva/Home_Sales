# BigData Challenge

## Description
This challenge uses Google Colab to run PySpark SQL Queries on a large dataset. Other methods are also used, for example Cache and Parquet to check if these queries are run faster or slower.

## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
- [Credits](#credits)

- [Discussion](#discussion)

- [Contact](#contact)

## Installation
You will need the following installations / coding skills to be able to replicate this project.

Google Colab, Python, PySpark

## Usage
    Please download the colab document and open Google Colab. From here, run the cells in sequential order.
    
## Credits
Jo Alva <br>

## Discussion
Parquet i.e. partitioning the data resulted in much faster times for the SQL queries to run.

## Contact
If there are any questions of concerns, I can be reached at:
##### [github: jojoalva](https://github.com/jojoalva)
##### [email: jyothsna_alva@hotmail.com](mailto:jyothsna_alva@hotmail.com)
